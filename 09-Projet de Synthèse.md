----
# **Projet Capstone en Data Engineering**
----

ğŸ¥‡ğŸ¥‡ğŸ¥‡ğŸ¥‡ğŸ¥‡ğŸ¥‡ğŸ¥‡ğŸ¥‡ğŸ¥‡ğŸ¥‡ğŸ¥‡ğŸ¥‡ğŸ¥‡ğŸ¥‡ğŸ¥‡ğŸ¥‡ğŸ¥‡ğŸ¥‡ğŸ¥‡ğŸ¥‡ğŸ¥‡ğŸ¥‡ğŸ¥‡ğŸ¥‡ğŸ¥‡ğŸ¥‡
----
*Partie1* 

----

# **AperÃ§u et objectifs**

Au cours de ce projet Capstone, vous allez crÃ©er une solution utilisant plusieurs services AWS que vous connaissez, sans recevoir des instructions dÃ©taillÃ©es Ã©tape par Ã©tape. Certaines sections de cet exercice sont conÃ§ues pour Ãªtre difficiles. Ce projet vous mettra au dÃ©fi de :

- Lancer et configurer un environnement de dÃ©veloppement intÃ©grÃ© (IDE) AWS Cloud9.
- Transformer des fichiers de donnÃ©es au format CSV en format Apache Parquet et les tÃ©lÃ©charger sur Amazon S3.
- CrÃ©er un crawler AWS Glue pour dÃ©duire la structure des donnÃ©es.
- Utiliser Amazon Athena pour interroger les donnÃ©es.
- CrÃ©er une vue Athena.
- Utiliser Amazon QuickSight pour visualiser les donnÃ©es.

# **DurÃ©e et suivi de votre budget**

Ce projet devrait nÃ©cessiter environ 4 heures pour Ãªtre complÃ©tÃ©. L'environnement est persistant : lorsque le minuteur de la session atteint 0:00, la session se termine, mais toutes les donnÃ©es et ressources crÃ©Ã©es dans le compte AWS sont conservÃ©es. Vous pouvez relancer la session Ã  tout moment avant la fin du temps imparti en choisissant "Start Lab".

# **AccÃ¨s aux services AWS**

L'accÃ¨s aux services et actions AWS peut Ãªtre restreint aux besoins de ce projet. Vous pourriez rencontrer des erreurs si vous tentez d'accÃ©der Ã  d'autres services ou d'exÃ©cuter des actions non dÃ©crites dans ce projet.

# **Le jeu de donnÃ©es**

Le site Sea Around Us propose un ensemble de donnÃ©es avec des informations historiques dÃ©taillÃ©es sur les pÃªcheries mondiales, de 1950 Ã  2018. Les donnÃ©es incluent les captures de poissons par pays, les tonnes pÃªchÃ©es, et la valeur de ces captures en dollars amÃ©ricains de 2010.

# **ScÃ©nario**

Vous devez crÃ©er l'infrastructure pour hÃ©berger ces donnÃ©es afin que les analystes puissent crÃ©er des rapports sur l'impact de la pÃªche en haute mer. Vous testerez cette infrastructure en utilisant trois fichiers de donnÃ©es du site Sea Around Us.

# **AccÃ¨s Ã  la console de gestion AWS**

1. **Lancer l'environnement de lab** :
   - Cliquez sur "Start Lab".
   - Attendez que l'icÃ´ne en haut Ã  gauche devienne verte avant de continuer.

2. **Connexion Ã  la console de gestion AWS** :
   - Cliquez sur le lien AWS en haut Ã  gauche pour ouvrir la console dans un nouvel onglet.

# **TÃ¢che 1 : Configuration de l'environnement de dÃ©veloppement**

1. **Observer les dÃ©tails du rÃ´le IAM CapstoneGlueRole**.
2. **CrÃ©er un environnement AWS Cloud9** :
   - Nommer l'environnement `CapstoneIDE`.
   - CrÃ©er une nouvelle instance EC2 t2.micro dans le VPC Capstone, sous-rÃ©seau public Capstone.
3. **CrÃ©er deux buckets S3** :
   - Dans la rÃ©gion us-east-1.
   - Nommer les buckets `data-source-#####` et `query-results-#####` oÃ¹ ##### est un numÃ©ro alÃ©atoire.

4. **TÃ©lÃ©charger les fichiers source .csv** dans l'IDE Cloud9 en utilisant les commandes `wget`.

5. **Observer les donnÃ©es** du fichier `SAU-GLOBAL-1-v48-0.csv` avec la commande `head -6 SAU-GLOBAL-1-v48-0.csv`.

6. **Convertir le fichier CSV en format Parquet** en utilisant pandas et pyarrow.

7. **TÃ©lÃ©charger le fichier Parquet dans le bucket S3 data-source**.

# **TÃ¢che 2 : Utilisation dâ€™un crawler AWS Glue et requÃªtes avec Athena**

1. **Configurer un crawler AWS Glue** :
   - CrÃ©er une base de donnÃ©es `fishdb` et un crawler `fishcrawler`.
   - Configurer le crawler pour utiliser le rÃ´le IAM `CapstoneGlueRole` et explorer le bucket `data-source`.

2. **ExÃ©cuter des requÃªtes SQL avec Athena** sur les donnÃ©es dÃ©couvertes.

# **TÃ¢che 3 : Transformation et ajout de nouveaux fichiers**

1. **Analyser la structure du fichier SAU-EEZ-242-v48-0.csv**.
2. **Modifier les noms de colonnes** pour les aligner avec les autres fichiers et convertir en format Parquet.
3. **TÃ©lÃ©charger le fichier transformÃ© dans le bucket S3**.
4. **Mettre Ã  jour la table dans AWS Glue** en rÃ©exÃ©cutant le crawler.

# **TÃ¢che 4 : Visualisation des rÃ©sultats dans QuickSight**

1. **CrÃ©er un compte QuickSight** et configurer l'accÃ¨s Ã  Athena.
2. **CrÃ©er un jeu de donnÃ©es QuickSight** basÃ© sur la vue `MackerelsCatch`.
3. **CrÃ©er un graphique dans QuickSight** reprÃ©sentant les tonnes de maquereaux pÃªchÃ©es par annÃ©e et par pays.

# **Diagramme ASCII du Pipeline**

```
    +--------------------+
    | Lancer AWS Cloud9  |
    +---------+----------+
              |
              v
    +--------------------+
    | CrÃ©er S3 Buckets   |
    +---------+----------+
              |
              v
    +---------------------------+
    | TÃ©lÃ©charger CSV dans Cloud9|
    +---------+------------------+
              |
              v
    +-----------------------------+
    | Convertir CSV en Parquet    |
    +---------+-------------------+
              |
              v
    +---------------------------+
    | TÃ©lÃ©charger Parquet sur S3 |
    +---------+------------------+
              |
              v
    +------------------------+
    | Configurer Glue Crawler|
    +---------+--------------+
              |
              v
    +-----------------------+
    | RequÃªtes avec Athena  |
    +---------+-------------+
              |
              v
    +------------------------+
    | Visualiser dans QuickSight |
    +------------------------+
```

ğŸ¥‡ğŸ¥‡ğŸ¥‡ğŸ¥‡ğŸ¥‡ğŸ¥‡ğŸ¥‡ğŸ¥‡ğŸ¥‡ğŸ¥‡ğŸ¥‡ğŸ¥‡ğŸ¥‡ğŸ¥‡ğŸ¥‡ğŸ¥‡ğŸ¥‡ğŸ¥‡ğŸ¥‡ğŸ¥‡ğŸ¥‡ğŸ¥‡ğŸ¥‡ğŸ¥‡ğŸ¥‡ğŸ¥‡
----
*Partie2 - Pipeline dÃ©taillÃ© du projet Capstone en Data Engineering**

----



# **1. Lancer et configurer l'environnement de dÃ©veloppement**

**Objectif :** CrÃ©er un environnement de dÃ©veloppement intÃ©grÃ© (IDE) sur AWS Cloud9 pour travailler sur ce projet.

1. **CrÃ©er un environnement Cloud9 :**
   - **Pourquoi ?** AWS Cloud9 est un IDE basÃ© sur le cloud qui vous permet d'Ã©crire, d'exÃ©cuter et de dÃ©boguer votre code Ã  partir de votre navigateur.
   - **Comment faire ?**
     - Allez sur la console AWS, puis recherchez "Cloud9".
     - Cliquez sur "Create environment".
     - Donnez un nom Ã  votre environnement, par exemple `CapstoneIDE`.
     - SÃ©lectionnez "Create a new EC2 instance" et choisissez une instance `t2.micro`, qui est gratuite dans le cadre de l'offre gratuite d'AWS.
     - Assurez-vous que l'instance est dÃ©ployÃ©e dans le VPC Capstone et dans le sous-rÃ©seau public Capstone.
     - Gardez toutes les autres options par dÃ©faut et cliquez sur "Create environment".

2. **VÃ©rifier les rÃ´les IAM :**
   - **Pourquoi ?** Le rÃ´le IAM `CapstoneGlueRole` permet Ã  diffÃ©rents services AWS de communiquer en toute sÃ©curitÃ© et d'accÃ©der aux ressources nÃ©cessaires.
   - **Comment faire ?**
     - Dans la console AWS, recherchez "IAM".
     - Allez dans "Roles" et recherchez `CapstoneGlueRole`.
     - VÃ©rifiez les permissions associÃ©es Ã  ce rÃ´le pour vous assurer qu'il peut accÃ©der aux ressources AWS nÃ©cessaires.

# **2. CrÃ©er et configurer les buckets S3**

**Objectif :** Stocker les fichiers de donnÃ©es dans Amazon S3, un service de stockage d'objets sÃ©curisÃ© et Ã©volutif.

1. **CrÃ©er deux buckets S3 :**
   - **Pourquoi ?** Les buckets S3 vous permettent de stocker et d'organiser les fichiers de donnÃ©es utilisÃ©s dans ce projet.
   - **Comment faire ?**
     - Dans la console AWS, recherchez "S3".
     - Cliquez sur "Create bucket".
     - Nommez le premier bucket `data-source-#####` (remplacez `#####` par un numÃ©ro alÃ©atoire).
     - Nommez le deuxiÃ¨me bucket `query-results-#####`.
     - SÃ©lectionnez la rÃ©gion `us-east-1` pour les deux buckets.
     - Gardez les paramÃ¨tres par dÃ©faut et cliquez sur "Create bucket".

# **3. TÃ©lÃ©charger les fichiers de donnÃ©es CSV**

**Objectif :** Obtenir les fichiers de donnÃ©es CSV nÃ©cessaires au projet et les examiner.

1. **TÃ©lÃ©charger les fichiers CSV :**
   - **Pourquoi ?** Ces fichiers contiennent des donnÃ©es historiques sur la pÃªche, que vous allez transformer et analyser.
   - **Comment faire ?**
     - Ouvrez le terminal dans votre environnement Cloud9.
     - ExÃ©cutez les commandes suivantes pour tÃ©lÃ©charger les fichiers CSV :

       ```bash
       wget https://aws-tc-largeobjects.s3.us-west-2.amazonaws.com/CUR-TF-200-ACDENG-1-91570/lab-capstone/s3/SAU-GLOBAL-1-v48-0.csv
       wget https://aws-tc-largeobjects.s3.us-west-2.amazonaws.com/CUR-TF-200-ACDENG-1-91570/lab-capstone/s3/SAU-HighSeas-71-v48-0.csv
       wget https://aws-tc-largeobjects.s3.us-west-2.amazonaws.com/CUR-TF-200-ACDENG-1-91570/lab-capstone/s3/SAU-EEZ-242-v48-0.csv
       ```

2. **Examiner les donnÃ©es :**
   - **Pourquoi ?** Comprendre la structure des fichiers CSV est essentiel pour les prochaines Ã©tapes de transformation.
   - **Comment faire ?**
     - Pour afficher les premiÃ¨res lignes d'un fichier, utilisez la commande `head` :

       ```bash
       head -6 SAU-GLOBAL-1-v48-0.csv
       ```

     - Cette commande affiche les en-tÃªtes de colonnes et les premiÃ¨res lignes de donnÃ©es, vous donnant un aperÃ§u de ce que contient le fichier.

# **4. Conversion des fichiers CSV en format Parquet**

**Objectif :** Convertir les fichiers CSV en format Parquet, un format de stockage optimisÃ© pour les gros volumes de donnÃ©es.

1. **Installer les outils nÃ©cessaires :**
   - **Pourquoi ?** Vous aurez besoin de bibliothÃ¨ques Python spÃ©cifiques pour effectuer la conversion.
   - **Comment faire ?**
     - Dans le terminal de Cloud9, installez les bibliothÃ¨ques avec la commande suivante :

       ```bash
       sudo pip3 install pandas pyarrow fastparquet
       ```

2. **Convertir le fichier CSV en Parquet :**
   - **Pourquoi ?** Le format Parquet est plus efficace pour le stockage et la requÃªte de donnÃ©es volumineuses.
   - **Comment faire ?**
     - DÃ©marrez une session Python interactive dans le terminal :

       ```bash
       python
       ```

     - Utilisez le code suivant pour lire le fichier CSV et le convertir en Parquet :

       ```python
       import pandas as pd
       df = pd.read_csv('SAU-GLOBAL-1-v48-0.csv')
       df.to_parquet('SAU-GLOBAL-1-v48-0.parquet')
       exit()
       ```

# **5. TÃ©lÃ©chargement des fichiers Parquet sur S3**

**Objectif :** Stocker les fichiers convertis dans le bucket S3.

1. **TÃ©lÃ©charger les fichiers Parquet sur S3 :**
   - **Pourquoi ?** Les fichiers Parquet doivent Ãªtre disponibles dans S3 pour Ãªtre analysÃ©s avec les autres services AWS.
   - **Comment faire ?**
     - Utilisez la commande AWS CLI suivante dans le terminal Cloud9 pour tÃ©lÃ©charger le fichier Parquet :

       ```bash
       aws s3 cp SAU-GLOBAL-1-v48-0.parquet s3://data-source-#####/
       ```

# **6. Utilisation d'un crawler AWS Glue et requÃªtes avec Athena**

**Objectif :** Configurer un crawler AWS Glue pour explorer les donnÃ©es et utiliser Amazon Athena pour interroger les donnÃ©es.

1. **Configurer un crawler AWS Glue :**
   - **Pourquoi ?** AWS Glue vous permet de cataloguer et prÃ©parer vos donnÃ©es pour l'analyse.
   - **Comment faire ?**
     - CrÃ©ez une base de donnÃ©es `fishdb` et un crawler `fishcrawler` dans AWS Glue.
     - Configurez le crawler pour utiliser le rÃ´le IAM `CapstoneGlueRole` et explorer le bucket `data-source`.

2. **ExÃ©cuter des requÃªtes avec Athena :**
   - **Pourquoi ?** Athena vous permet de faire des requÃªtes SQL directement sur vos donnÃ©es stockÃ©es dans S3.
   - **Comment faire ?**
     - AccÃ©dez Ã  Athena via la console AWS, configurez l'Ã©diteur de requÃªtes pour stocker les rÃ©sultats dans votre bucket `query-results`.
     - ExÃ©cutez une requÃªte pour vÃ©rifier que vos donnÃ©es sont correctement cataloguÃ©es.

# **7. Visualisation des rÃ©sultats avec Amazon QuickSight**

**Objectif :** CrÃ©er des visualisations pour analyser les rÃ©sultats de vos requÃªtes.

1. **CrÃ©er un compte QuickSight :**
   - **Pourquoi ?** QuickSight est un outil de visualisation de donnÃ©es qui vous permet de crÃ©er des rapports interactifs.
   - **Comment faire ?**
     - Allez sur la console QuickSight, inscrivez-vous et configurez l'accÃ¨s Ã  Athena.

2. **CrÃ©er des visualisations :**
   - **Pourquoi ?** Pour transformer les donnÃ©es en graphiques et rapports comprÃ©hensibles.
   - **Comment faire ?**
     - CrÃ©ez un nouveau jeu de donnÃ©es Ã  partir de la vue `MackerelsCatch`.
     - CrÃ©ez un graphique reprÃ©sentant les tonnes de maquereaux pÃªchÃ©es par annÃ©e et par pays.

# **Diagramme du Pipeline**


```
      +-------------------------------------+
      | 1. Lancer AWS Cloud9 IDE            |
      +------------------+------------------+
                         |
                         v
      +-------------------------------------+
      | 2. CrÃ©er S3 Buckets                 |
      +------------------+------------------+
                         |
                         v
      +-------------------------------------+
      | 3. TÃ©lÃ©charger fichiers CSV         |
      +------------------+------------------+
                         |
                         v
      +-------------------------------------+
      | 4. Conversion CSV -> Parquet        |
      +------------------+------------------+
                         |
                         v
      +-------------------------------------+
      | 5. TÃ©lÃ©charger Parquet sur S3       |
      +------------------+------------------+
                         |
                         v
      +-------------------------------------+
      | 6. Configurer AWS Glue Crawler      |
      +------------------+------------------+
                         |
                         v
      +-------------------------------------+
      | 7. RequÃªtes SQL avec Athena         |
      +------------------+------------------+
                         |
                         v
      +-------------------------------------+
      | 8. Visualisation avec QuickSight    |
      +-------------------------------------+
```

ğŸ¥‡ğŸ¥‡ğŸ¥‡ğŸ¥‡ğŸ¥‡ğŸ¥‡ğŸ¥‡ğŸ¥‡ğŸ¥‡ğŸ¥‡ğŸ¥‡ğŸ¥‡ğŸ¥‡ğŸ¥‡ğŸ¥‡ğŸ¥‡ğŸ¥‡ğŸ¥‡ğŸ¥‡ğŸ¥‡ğŸ¥‡ğŸ¥‡ğŸ¥‡ğŸ¥‡ğŸ¥‡ğŸ¥‡
----
*Partie3 - Explication des diffÃ©rents composants**

----

